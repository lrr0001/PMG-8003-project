\documentclass[11pt]{article}
\usepackage{fontspec}
\usepackage{ amssymb }
\usepackage{longtable}
\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\author{Raghav Kuppan}
\title{Project Proposal}
\begin{document}

\begin{center}
\textbf{\Large Learning the structure of a Markov Random Field}
\end{center}

\begin{center}
Lee Richert (ECE), Raghav Kuppan (ECE), Yuanda Zhu (ECE)
\end{center}



\section{Abstract}
Learning the structure of a Markov Random Field from independent and identically distributed samples is an important problem complicated by the fact that calculation of the partition function is intractable for most graphs. Trace LASSO regularization has been proposed to solve this problem as it exhibits less volatility in response to correlation between random variables than LASSO while also promoting sparsity. We plan to adapt Trace LASSO for the purpose of learning the structure of a Markov Random field, using a Maximum Likelihood approach.

\section{Introduction}
In many modern applications, the number of observations is much less than the number of features which in the case of graphical models is the number of nodes in the graph. In such cases, finding a consistent estimator is difficult owing to the intractability of the partition function calculation. One problem of interest is that of model selection for Ising models which are a special case of pairwise Markov Random fields. In an Ising model, the full conditional distributions of each variable form logistic regression models, and variable selection techniques for regression allow one to identify the neighborhood of each node and, thus, the entire graph. Prior work on this has involved solving this problem or a variant of this problem using a Maximum likelihood approach with $\ell_1$-regularization ~\cite{ravikumar2010high}, Bayesian Information Criteria~\cite{barber2015high} or Local Separation Criteria~\cite{anandkumar2012high}. \\ \\
Alternate methods of regularization have been developed in recent years to promote sparsity. Among sparsity inducing norms, the $\ell_1$ norm is the simplest and most widely used, leading to the Lasso when used in a least-squares framework. While the Lasso does well in high-dimensional settings, it is known to have stability problems in situations where the data exhibit strong correlation structures. Several solutions have been proposed which include the Elastic net, group Lasso and Sampling techniques. However these norms cannot just be plugged into the objective function as extra information is usually required, or in some cases, the problem is further complicated due to the addition of parameters to be estimated. The Trace norm takes into account the correlation structure of the data but does not require manual human intervention. It can be thought of as a way of turning the rank of a matrix into a norm. The Trace norm is adaptive and requires that only a single regularization parameter be chosen. 	

\section{Prior Work}
Ravikumar et. al ~\cite{ravikumar2010high} study the problem of signed edge recovery on Ising models and establish sufficient conditions on the sample size, dimension, and maximum neighborhood size, to get a consistent estimator. When these conditions are met, the signed neighborhood of every node can be estimated by optimizing a regularized log likelihood function. This way the entire set of signed edges of the graph can be recovered whereby the structure of the Ising model has been found. 
\section{Approach}

\section{Prior Work on Regularization}

Regularization as well as variable selection plays a major role in linear regression. A large number of proposals are made to overcome the limitation of least square regression. The early milestone in this area is ridge regression ~\cite{AEHoerl1970ridge}. Subjected to a L2 penalty, ridge regression aims to minimize the sum of squared error. Ridge regression achieves better performance through bias-variance trade-off. However, the main drawback is that ridge regression is not a parsimonious model; it simply keeps all predictors in the model. 
\\ \\
Another popular algorithm, lasso ~\cite{tibshirani1996regression}, was proposed in 1996. By incorporating the L1-norm for regularization, lasso performs optimally in high-dimensional, low-correlated settings, in terms of both prediction and parameter estimation. Unlike ridge regression, lasso does have sparse representation. People have figured out that, nevertheless, lasso performs poorly under three conditions: 1) Let p represent the number of features and n represent the number of observations. For p >n, lasso selects at most n variables before it saturates; besides, lasso is not well defined unless the bound on L1-norm of the coefficients is smaller than a certain value. 2) For a group of variables whose pairwise correlations are very high, lasso tends to select only one variable and does not care which one is selected. 3) For n>p case, when predictors have high correlations, the prediction performance of lasso is dominated by ridge regression.
\\ \\
In order to address the third problem of lasso, elastic net ~\cite{Zou2005Reg} was proposed in 2005 by adding the squared L2 norm, a strongly convex penalty term to the L1 norm in lasso. Elastic net performs similarly to lasso in scenario 1) and 2), but by encouraging grouping effect, has higher accuracy than lasso in scenario 3). To be more specific, in scenario 3), some features are highly correlated with each other and are associated with response; thus elastic net aims to perform less shrinkage on those subsets of features. In addition, similar to lasso, elastic net does both continuous shrinkage and automatic variable selection. Note that ridge regression has only continuous shrinkage but no automatic variable selection.
\\ \\
Group lasso ~\cite{Francis2008Con} is another approach to implement the grouping effect to lasso. Group lasso divides predictors into group and penalizes the sum of L2-norm. The basic assumption of group lasso is known a priori that there are distinct groups or clusters among variables; it uses L2 penalty on coefficients within each of K known and non-overlapping groups. However, as many researchers pointed out, knowing the group in advance is not always possible.
\\ \\
On top of group lasso and elastic net, clustering algorithm is also involved to each of them. Cluster group lasso ~\cite{Peter2013Cor} seeks sets of correlated features with similar associations with the response. The idea is to first identify groups among features using hierarchical clustering and then apply group lasso. The basic assumption is that all correlated features have similar association with response. If this assumption fails to hold so that not all correlated features have a similar association with the response, then cluster elastic net (CEN) ~\cite{Dan2014The} would show its advantage since CEN seeks sets of correlated features with similar associations with the response. Besides, while elastic net shrinks all coefficients towards the origin, CEN selectively shrinks coefficients for highly-correlated variables towards each other. 

\section{Summary}

\bibliography{citation}{}
\bibliographystyle{ieeetr}

\end{document}
