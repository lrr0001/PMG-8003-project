\documentclass[11pt]{article}
%\usepackage{fontspec}
\usepackage{ amssymb }
\usepackage{longtable}
\usepackage{amsmath}
%\usepackage[utf8]{inputenc}
\author{Raghav Kuppan}
\title{Project Proposal}
\begin{document}

\begin{center}
\textbf{\Large Learning the structure of a Markov Random Field}
\end{center}

\begin{center}
Lee Richert (ECE), Raghav Kuppan (ECE), Yuanda Zhu (ECE)
\end{center}



\section{Abstract}
Learning the structure of a Markov Random Field from independent and identically distributed samples is an important problem complicated by the fact that calculation of the partition function is intractable for most graphs. Trace Lasso regularization has been proposed to solve this problem as it exhibits less volatility in response to correlation between random variables than Lasso while also promoting sparsity. We plan to adapt Trace Lasso for the purpose of learning the structure of a Markov Random field, using a Maximum Likelihood approach.

\section{Introduction}
In many modern applications, the number of observations is much less than the number of features which in the case of graphical models is the number of nodes in the graph. In such cases, finding a consistent estimator is difficult owing to the intractability of the partition function calculation. One problem of interest is that of model selection for Ising models which are a special case of pairwise Markov Random fields. In an Ising model, the full conditional distributions of each variable form logistic regression models, and variable selection techniques for regression allow one to identify the neighborhood of each node and, thus, the entire graph. Prior work on this has involved solving this problem or a variant of this problem using a Maximum likelihood approach with $\ell_1$-regularization ~\cite{ravikumar2010high}, Bayesian Information Criteria~\cite{barber2015high} or Local Separation Criteria~\cite{anandkumar2012high}.\\

Alternate methods of regularization have been developed in recent years to promote sparsity. Among sparsity inducing norms, the $\ell_1$ norm is the simplest and most widely used, leading to the Lasso when used in a least-squares framework. While the Lasso does well in high-dimensional settings, it is known to have stability problems in situations where the data exhibit strong correlation structures. Several solutions have been proposed which include the Elastic net, group Lasso and Sampling techniques. However these norms cannot just be plugged into the objective function as extra information is usually required, or in some cases, the problem is further complicated due to the addition of parameters to be estimated. The Trace norm takes into account the correlation structure of the data but does not require manual human intervention. It can be thought of as a way of turning the rank of a matrix into a norm. The Trace norm is adaptive and requires that only a single regularization parameter be chosen. 	

\section{Prior Work on Model Selection}
Ravikumar et. al ~\cite{ravikumar2010high} study the problem of signed edge recovery on Ising models and establish sufficient conditions on the sample size $n$, dimension $p$, and maximum neighborhood size $d$, to get a consistent estimator.In a high dimensional setting, both $p$ and $d$ grow as a function of $n$. With this assumption, the structure of any bounded degree graph can be recovered with high probability once $n/log(p)$ is sufficiently large. This way, the signed neighborhood of every node can be estimated by optimizing a log likelihood function with an $\ell_1$ penalty. This way the entire set of signed edges of the graph can be recovered whereby the structure of the Ising model has been found. This technique is demonstrated on four-nearest neighbor lattices, eight-nearest neighbor lattices and a star graph as well as on a class of graphs with unbounded maximum neighborhood size, with the results being consistent with the theoretical conjectures. Methods to generalize the technique to any discrete pairwise Markov random field are discussed.\\

Unlike the Ravikumar et. al method described above and our proposed approach, the conditiona variational distance thresholding algorithm (CVDT) \cite{anandkumar2012high} does not use a pseudo-likelihood approach.  Instead, it uses the conditional variational distance as an indicator of dependence. In addition to conditions requiring sufficient samples and some graph-family-specific bounds on edge potentials, the CVDT also requires that the graph structure meet a $(\eta,\gamma)$-local seperation property: for every pair of nodes, there exists a set $S: |S| \leq \eta$ of cardinality less than or equal to $\eta$ such that in the graph $G' = G\S$, $d(X_i,X_j) > \gamma$. $\eta$ is a hyperparameter used in the algorithm, and $\gamma$ is required to be at least on the order of $\log(\log(p))$ where $p$ is the number of nodes in the graph. Details of sufficient requirements for recovery of graph structure can be found in ~\cite{anandkumar2012high}.

The CVDT algorithm considers each pair of nodes $X_i,X_j$ to determine whether the edge between them should be included.  The edge is included if there does not exist an assignment of values for any subset $S \subseteq X\\\{X_i,X_j\}$ with $|S| \leq \eta$ such that the probability mass function for $X_i$ does not sufficiently change when conditioned on different values of $X_j$.


Checking all potential value assignments for all subsets $\{S_i\}$ grows exponentially with the size of $\eta$, so to be computationally feasible, $\eta$ cannot be large. 




\section{Prior Work on Regularization}

Regularization as well as variable selection plays a major role in linear regression. A large number of proposals are made to overcome the limitation of least square regression. The early milestone in this area is ridge regression ~\cite{AEHoerl1970ridge}. Subjected to a L2 penalty, ridge regression aims to minimize the sum of squared error. Ridge regression achieves better performance through bias-variance trade-off. However, the main drawback is that ridge regression is not a parsimonious model; it simply keeps all predictors in the model. 
\\ \\
Another popular algorithm, lasso ~\cite{tibshirani1996regression}, was proposed in 1996. By incorporating the L1-norm for regularization, lasso performs optimally in high-dimensional, low-correlated settings, in terms of both prediction and parameter estimation. Unlike ridge regression, lasso does have sparse representation. People have figured out that, nevertheless, lasso performs poorly under three conditions:

\begin{enumerate} 

\item

Let $p$ represent the number of features and $n$ represent the number of observations. For $p>n$, lasso selects at most $n$ variables before it saturates; besides, lasso is not well defined unless the bound on L1-norm of the coefficients is smaller than a certain value.
\item

For a group of variables whose pairwise correlations are very high, lasso tends to select only one variable and does not care which one is selected.

\item

 For $n>p$ case, when predictors have high correlations, the prediction performance of lasso is dominated by ridge regression.\\ \\

\end{enumerate}

In order to address the third problem of lasso, elastic net ~\cite{Zou2005Reg} was proposed in 2005 by adding the squared L2 norm, a strongly convex penalty term to the L1 norm in lasso. Elastic net performs similarly to lasso in scenario 1) and 2), but by encouraging grouping effect, has higher accuracy than lasso in scenario 3). To be more specific, in scenario 3), some features are highly correlated with each other and are associated with response; thus elastic net aims to perform less shrinkage on those subsets of features. In addition, similar to lasso, elastic net does both continuous shrinkage and automatic variable selection. Note that ridge regression has only continuous shrinkage but no automatic variable selection.
\\ \\
Group lasso ~\cite{Francis2008Con} is another approach to implement the grouping effect to lasso. Group lasso divides predictors into group and penalizes the sum of L2-norm. The basic assumption of group lasso is known a priori that there are distinct groups or clusters among variables; it uses L2 penalty on coefficients within each of K known and non-overlapping groups. However, as many researchers pointed out, knowing the group in advance is not always possible.
\\ \\
On top of group lasso and elastic net, clustering algorithm is also involved to each of them. Cluster group lasso ~\cite{Peter2013Cor} seeks sets of correlated features with similar associations with the response. The idea is to first identify groups among features using hierarchical clustering and then apply group lasso. The basic assumption is that all correlated features have similar association with response. If this assumption fails to hold so that not all correlated features have a similar association with the response, then cluster elastic net (CEN) ~\cite{Dan2014The} would show its advantage since CEN seeks sets of correlated features with similar associations with the response. Besides, while elastic net shrinks all coefficients towards the origin, CEN selectively shrinks coefficients for highly-correlated variables towards each other.

\section{Prior Work on Hyperparameter Selection and Model Fusion}
Many appoaches to solve ising selection require the user to select at least one hyperparameter.  Difference hyperparameter selections often produce different solutions.  One of the most popular approaches to hyperparameter selection is cross validation. Cross validation is a method of choosing one of several candidate hyperparameter selections.  In cross validation, the training data $X$ is divided into K paritions $\{P_1,P_2,\hdots,P_K\}$ of approximately equal size.

$$P_1 \cup P_2 \cup \hdots \cup P_K = X$$

$$P_i \cap P_j = \empty \forall i \neq j$$

For each hyerparameter selection candidate $\lambda_i$ and each partition $P_k$, the algorithm is trained on all of the training data that is not in the partition $X\P_i$ and then evaluated on the partition $P_k$. The hyperparameter selection candidate with the best performance averaged across all partitions is chosen. Then, the model can be retrained using the selected hyperparameters on the entire training set.

Baysian Information Criterion (BIC) \cite{Schwarz1978estimating} can sometimes be used as an alternative to cross-validation. BIC offers a means to compare different probabilistic models that use different numbers of parameters. Complex models (with more parameters) can achieve higher likelihoods by overfitting the data. The BIC adds a term to the negative logarithmic likelihood function to penalize model complexity. The BIC formula is
$$\operatorname{BIC}_0(\theta_J)  = -2\log(P(X|\theta_J)) + |J|\log(n)$$
where $n$ is the number of samples, $J$ is the set of nonzero parameters, and $\theta_J$ is a vector containing the parameters of the model.
In LASSO regression, increasing $\lambda$ tends to decrease the number of nonzero parameters, decreasing model complexity. So, BIC can be used to select $\lambda$ in LASSO regression. Researchers have found that in the high-dimensional setting, the standard BIC tends to under-penalize model complexity, so in this setting a modified BIC is more appropriate. There are many BIC variants \cite{zak2011modified}. In \cite{barber2015high}, Barber and Drton use a modified BIC for model selection to select the $\lambda$ hyperparameter in LASSO to learn Ising models from data.
$$BIC_{\gamma}(\theta_J) = -2\log(P(X|\theta_J)) + |J|(\log(n) + \gamma\log(p))$$
where $\gamma$ is specified by the user, and $p$ is the number of nodes in the graph. Unfortunately, this adds another hyperparameter.  However, results may be less sensative to choice of $\gamma$ than other hyperparameters.

Another option available to us is stability selection \cite{meinshausen2010stability}. Stability selection does not attempt to choose a particular hyperparameter selection candidate.  Instead, it constructs a new model from the many models generated from different hyperparameter selections trained on various subsamples of the training set. For each hyperparameter selection candidate, models are trained on subsets of the training data. For any hyperparameter choice in the set of hyperparameter selection candidates, if the fraction of models that assign a nonzero value to a particular parameter exceeds a threshold, that parameter is included in the final model. This threshold is a hyperparameter the user will need to specify.
\section{Approach}
Our approach is to adopt the maximum likelihood approach as in~\cite{ravikumar2010high} but with the trace norm as our regularization parameter. The signed neighborhood of one node will be estimated by evaluating the conditional distribution of the node conditioned on all the other nodes and then maximizing this with a trace norm penalty. Computing the subgradient of the trace norm is reported to be inefficient and the rate of convergence very slow, so a variational formulation for the trace norm~\cite{grave2011trace} will be used. The ensuing cost function can be optimized using a Conjugated Gradients method. \\ \\ 
 
\subsection{Approach for Evaluating Performance}
To test different approaches for the ising-selection problem, we will first generate sythetic data. Data samples can be generated from a known probabilistic graphical model using Gibbs sampling. Samples drawn from different runs after sufficient burn time will be independent and identically distributed.  From these samples, we can use structural learning algorithms described in prior research and our proposed methods to learn a new graphical model, which can be directly compared with the known model for accuracy. We plan to evaluate the algorithms by counting the number of edges whose sign is recovered correctly.
$$\operatorname{sign}(\theta_i) = \begin{cases} 1 & \theta_i > 0\\ 0 & \theta_i = 0 \\ -1 & \theta_i < 0 \end{cases}$$

On real data for which the generating graphical model is unknown, we can evaluate algorithms by measuring the likelihood on a hold-out set.
\section{Summary}

\bibliography{citation}{}
\bibliographystyle{plain}

\end{document}

