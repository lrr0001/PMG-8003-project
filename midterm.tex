\documentclass[11pt]{article}
\usepackage{fontspec}
\usepackage{ amssymb }
\usepackage{longtable}
\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\author{Raghav Kuppan}
\title{Project Proposal}
\begin{document}

\begin{center}
\textbf{\Large Learning the structure of a Markov Random Field}
\end{center}

\begin{center}
Lee Richert (ECE), Raghav Kuppan (ECE), Yuanda Zhu (ECE)
\end{center}



\section{Abstract}
Learning the structure of a Markov Random Field from independent and identically distributed samples is an important problem complicated by the fact that calculation of the partition function is intractable for most graphs. Trace LASSO regularization has been proposed to solve this problem as it exhibits less volatility in response to correlation between random variables than LASSO while also promoting sparsity. We plan to adapt Trace LASSO for the purpose of learning the structure of a Markov Random field, using a Maximum Likelihood approach.

\section{Introduction}
In many modern applications, the number of observations is much less than the number of features which in the case of graphical models is the number of nodes in the graph. In such cases, finding a consistent estimator is difficult owing to the intractability of the partition function calculation. One problem of interest is that of model selection for Ising models which are a special case of pairwise Markov Random fields. In an Ising model, the full conditional distributions of each variable form logistic regression models, and variable selection techniques for regression allow one to identify the neighborhood of each node and, thus, the entire graph. Prior work on this has involved solving this problem or a variant of this problem using a Maximum likelihood approach with $\ell_1$-regularization ~\cite{ravikumar2010high}, Bayesian Information Criteria~\cite{barber2015high} or Local Separation Criteria~\cite{anandkumar2012high}. \\ \\
Alternate methods of regularization have been developed in recent years to promote sparsity. Among sparsity inducing norms, the $\ell_1$ norm is the simplest and most widely used, leading to the Lasso when used in a least-squares framework. While the Lasso does well in high-dimensional settings, it is known to have stability problems in situations where the data exhibit strong correlation structures. Several solutions have been proposed which include the Elastic net, group Lasso and Sampling techniques. However these norms cannot just be plugged into the objective function as extra information is usually required, or in some cases, the problem is further complicated due to the addition of parameters to be estimated. The Trace norm takes into account the correlation structure of the data but does not require manual human intervention. It can be thought of as a way of turning the rank of a matrix into a norm. The Trace norm is adaptive and requires that only a single regularization parameter be chosen. 	

\section{Prior Work}
Ravikumar et. al ~\cite{ravikumar2010high} study the problem of signed edge recovery on Ising models and establish sufficient conditions on the sample size, dimension, and maximum neighborhood size, to get a consistent estimator. When these conditions are met, the signed neighborhood of every node can be estimated by optimizing a regularized log likelihood function. This way the entire set of signed edges of the graph can be recovered whereby the structure of the Ising model has been found. 
\section{Approach}

\section{Results of Prior Work}

\section{Summary}

\bibliography{citation}{}
\bibliographystyle{plain}

\end{document}
